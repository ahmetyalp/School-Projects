{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_PATH = '../glove.6B.200d.pkl'\n",
    "ARTICLES_PATH = '../articles/'\n",
    "GOLD_SUMMARIES_PATH = '../gold_summaries/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes ~37 seconds in my computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from rouge import Rouge \n",
    "import collections, functools, operator \n",
    "\n",
    "ALL_FILES = os.listdir(os.path.abspath( os.path.join( os.getcwd() , ARTICLES_PATH )))\n",
    "\n",
    "vec = pickle.load(open(VECTOR_PATH , 'rb'))\n",
    "\n",
    "punc = {'£': ' ' , '\"': ' ', '!': ' ', '^': ' ', '%': ' ', '<': ' ', '+': ' ', '~': ' ', '*': ' ', ';': ' ', ':': ' ', '(': ' ', '?': ' ', '&': ' ', '}': ' ', ']': ' ', '|': ' ', ',': ' ', \"'\": ' ', ')': ' ', '-': ' ', '#': ' ', '`': ' ', '@': ' ', '/': ' ', '$': ' ', '_': ' ', '{': ' ', '.': ' ', '>': ' ', '[': ' ', '\\\\': ' ', '=': ' '}\n",
    "punc = str.maketrans(punc)\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "def preprocess(string): # Removes punctuations, replaces them with space character\n",
    "    global punc\n",
    "    string = string.translate(punc)\n",
    "    return string.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_documents_to_sentences():\n",
    "    docs_by_sentences = []\n",
    "    for filename in file_list: # Read every document\n",
    "        f = open(os.path.abspath(os.path.join(os.getcwd() , ARTICLES_PATH , filename)) , 'r')\n",
    "        title = f.readline() # Get title as a sentence\n",
    "        f = f.read() # Read rest of the document\n",
    "        sentences = [title.strip()]\n",
    "        sentences.extend(split_into_sentences(f)) # Extract sentences\n",
    "        docs_by_sentences.append(sentences)\n",
    "    return docs_by_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use two different model when representing sentences. First model simply take avarages of words while second one considers in term frequency for weighted avarage. Below one is the simple one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(docs_by_sentences):\n",
    "    docs_as_vectors = []\n",
    "    for doc in docs_by_sentences:\n",
    "        vectors = []\n",
    "        for sentence in doc:\n",
    "            vector = np.zeros(200) # initialize vector represents the sentece\n",
    "            number_of_words = 0 # Number of valid words in the sentence\n",
    "            for word in preprocess(sentence).split():\n",
    "                if vec.get(word) is not None: # If the word has a vector representation \n",
    "                    vector = vector + vec[word]\n",
    "                    number_of_words += 1\n",
    "            if number_of_words == 0: # If there is no valid word in sentence, for example there are sentences like '.' because of edge cases in preprocess, simply ignore it\n",
    "                vectors.append(\"PASS\")\n",
    "                #print(\"Oopss this is not a sentence: \" ,  sentence)\n",
    "            else:\n",
    "                vectors.append(vector/number_of_words) # Sentence is avarage of words\n",
    "        docs_as_vectors.append(vectors)\n",
    "    return docs_as_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is model with tf's. I multiply every word vector to its term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sent2vec(docs_by_sentences):\n",
    "#     docs_as_vectors = []\n",
    "#     for doc in docs_by_sentences:\n",
    "#         tf = {}\n",
    "#         for sentence in doc:\n",
    "#             for word in preprocess(sentence).split():\n",
    "#                 if vec.get(word) is not None:\n",
    "#                     tf[word] = tf.get(word , 0.0) + 1.0\n",
    "#         vectors = []\n",
    "#         total_words = sum(tf.values())\n",
    "#         for sentence in doc:\n",
    "#             vector = np.zeros(200) # initialize vector represents the sentece\n",
    "#             number_of_words = 0\n",
    "#             for word in preprocess(sentence).split():\n",
    "#                 if vec.get(word) is not None: # If the word has a vector representation \n",
    "#                     vector = vector + np.multiply(vec[word] ,  (tf[word]/total_words))\n",
    "#                     number_of_words += 1\n",
    "#             if number_of_words == 0: # If there is no valid word in sentence, for example there are sentences like '.' because of edge cases in preprocess, simply ignore it\n",
    "#                 vectors.append(\"PASS\")\n",
    "#                 #print(\"Oopss this is not a sentence: \" ,  sentence)\n",
    "#             else:\n",
    "#                 vectors.append(vector) # Sentence is avarage of words\n",
    "#         docs_as_vectors.append(vectors)\n",
    "#     return docs_as_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use static K for K-Means. I use first K point as initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(docs_as_vectors):\n",
    "    extracted_summaries = []\n",
    "    for i in range(len(docs_as_vectors)):\n",
    "        #print(file_list[i])\n",
    "        sentences = kmean(docs_as_vectors[i]) # Take positions from k mean\n",
    "        summary = \"\"\n",
    "        for sentence in sentences:\n",
    "            summary += docs_by_sentences[i][sentence] # Get sentences from document and create summary\n",
    "            summary += \" \"\n",
    "        extracted_summaries.append(summary.strip())\n",
    "    return extracted_summaries\n",
    "\n",
    "def kmean(vectors): # Returns the positions of sentences that will be used in summary for a document\n",
    "    K = 3\n",
    "    count = 0\n",
    "    means = []\n",
    "    for i in range(len(vectors)): # Initial means, I choose first K, skip empty sentences\n",
    "        if type(vectors[i]) == str:\n",
    "            continue\n",
    "        count += 1\n",
    "        means.append(vectors[i])\n",
    "        if count == K:\n",
    "            break\n",
    "    clusters = np.zeros(len(vectors) , dtype=np.uint8) # clusters[i] means sentence i is in cluster[i]\n",
    "    for i in range(len(vectors)):\n",
    "        vector = vectors[i]\n",
    "        if type(vector) == str: # I push \"PASS\" as vector for invalid sentences so I check if it a vector or string\n",
    "            continue\n",
    "        clusters[i] = np.argmin([np.linalg.norm(vector - mean) for mean in means]) + 1 # assign each sentence a new cluster according current means\n",
    "    new_mean = [np.mean([vectors[i] for i in range(len(vectors)) if clusters[i] == k] , axis=0) for k in range(1,K+1)] # Re-calculate current means of clusters\n",
    "    while np.mean([np.linalg.norm(means[i] - new_mean[i]) for i in range(len(means))]) > 0.000001: # Until means don't change much\n",
    "        #print(\"step ..\")\n",
    "        means = new_mean\n",
    "        clusters = np.zeros(len(vectors) , dtype=np.uint8)\n",
    "        for i in range(len(vectors)):\n",
    "            vector = vectors[i]\n",
    "            if type(vector) == str:\n",
    "                continue\n",
    "            clusters[i] = np.argmin([np.linalg.norm(vector - mean) for mean in means]) + 1 # Reassign sentences to new clusters\n",
    "        new_mean = [np.mean([vectors[i] for i in range(len(vectors)) if clusters[i] == k] , axis=0) for k in range(1,K+1)] # Recalculate cluster's means\n",
    "    means = new_mean\n",
    "    #print(clusters)\n",
    "    res = []\n",
    "    for i in range(1,K+1): # Find the sentence that is closest to the mean for every cluster and choose them as summary sentences\n",
    "        pos = -1\n",
    "        dist = np.inf\n",
    "        for j in range(0,len(clusters)):\n",
    "            if clusters[j] == i:\n",
    "                if np.linalg.norm(vectors[j] - means[i-1]) < dist:\n",
    "                    dist = np.linalg.norm(vectors[j] - means[i-1])\n",
    "                    pos = j\n",
    "        res.append(pos)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(gold_summaries, extracted_summaries):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(extracted_summaries , gold_summaries , avg=True)\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_fold = np.array_split(ALL_FILES , 10) # K-fold, I use k = 10. Use first fold as test, others are for training\n",
    "\n",
    "file_list = files_fold[0]\n",
    "docs_by_sentences = parse_documents_to_sentences()\n",
    "docs_as_vectors = sent2vec(docs_by_sentences)\n",
    "extracted_summaries = cluster(docs_as_vectors)\n",
    "\n",
    "gold_summaries = []\n",
    "for filename in file_list:\n",
    "    f = open(os.path.abspath(os.path.join(os.getcwd() , GOLD_SUMMARIES_PATH , filename)) , 'r').read()\n",
    "    gold_summaries.append(f)\n",
    "\n",
    "scores = evaluation(gold_summaries , extracted_summaries)\n",
    "print(\"------ TEST SCORES ------\")\n",
    "print(scores)\n",
    "# Calculate validation scores\n",
    "validate_scores_one = [] \n",
    "validate_scores_two = []\n",
    "validate_scores_l = []\n",
    "for file_list in files_fold[1:]:\n",
    "    docs_by_sentences = parse_documents_to_sentences()\n",
    "    docs_as_vectors = sent2vec(docs_by_sentences)\n",
    "    extracted_summaries = cluster(docs_as_vectors)\n",
    "\n",
    "    gold_summaries = []\n",
    "    for filename in file_list:\n",
    "        f = open(os.path.abspath(os.path.join(os.getcwd() , GOLD_SUMMARIES_PATH , filename)) , 'r').read()\n",
    "        gold_summaries.append(f)\n",
    "\n",
    "    scores = evaluation(gold_summaries , extracted_summaries)\n",
    "    validate_scores_one.append(scores['rouge-1'])\n",
    "    validate_scores_l.append(scores['rouge-2'])\n",
    "    validate_scores_two.append(scores['rouge-l'])\n",
    "\n",
    "validate_means = { 'rouge-1': dict(functools.reduce(operator.add, \n",
    "         map(collections.Counter, validate_scores_one))),\n",
    "            'rouge-2': dict(functools.reduce(operator.add, \n",
    "         map(collections.Counter, validate_scores_two))),\n",
    "            'rouge-l': dict(functools.reduce(operator.add, \n",
    "         map(collections.Counter, validate_scores_l)))}\n",
    "\n",
    "for key in validate_means.keys(): # First I calculate sums, now I divide all values to 9 to calculate mean\n",
    "    for key_ in validate_means[key].keys():\n",
    "        validate_means[key][key_] = validate_means[key][key_] / 9.0\n",
    "\n",
    "print(\"------ VALIDATION SCORES ------\")\n",
    "print(\"means ==> \" , validate_means)\n",
    "\n",
    "validate_var = {}\n",
    "squares = [] # Stores squares of (val - mean)**2, them sum all of them and divide 9 to calculate variation\n",
    "mean = validate_means['rouge-1']\n",
    "for score in validate_scores_one:\n",
    "    squares.append({\n",
    "        'f': (score['f'] - mean['f'])**2,\n",
    "        'p': (score['p'] - mean['p'])**2,\n",
    "        'r': (score['r'] - mean['r'])**2\n",
    "    })\n",
    "validate_var['rouge-1'] = dict(functools.reduce(operator.add, \n",
    "         map(collections.Counter, squares)))\n",
    "squares = []\n",
    "mean = validate_means['rouge-2']\n",
    "for score in validate_scores_one:\n",
    "    squares.append({\n",
    "        'f': (score['f'] - mean['f'])**2,\n",
    "        'p': (score['p'] - mean['p'])**2,\n",
    "        'r': (score['r'] - mean['r'])**2\n",
    "    })\n",
    "validate_var['rouge-2'] = dict(functools.reduce(operator.add, \n",
    "         map(collections.Counter, squares)))\n",
    "squares = []\n",
    "mean = validate_means['rouge-l']\n",
    "for score in validate_scores_one:\n",
    "    squares.append({\n",
    "        'f': (score['f'] - mean['f'])**2,\n",
    "        'p': (score['p'] - mean['p'])**2,\n",
    "        'r': (score['r'] - mean['r'])**2\n",
    "    })\n",
    "validate_var['rouge-l'] = dict(functools.reduce(operator.add, \n",
    "         map(collections.Counter, squares)))\n",
    "\n",
    "for key in validate_var.keys():\n",
    "    for key_ in validate_var[key].keys():\n",
    "        validate_var[key][key_] = validate_var[key][key_] / 9.0\n",
    "\n",
    "print(\"variations ==> \" , validate_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: <br>\n",
    "------ TEST SCORES ------ <br>\n",
    "{'rouge-1': {'f': 0.4803040479084102, 'p': 0.7232442115995087, 'r': 0.36760463219012707}, 'rouge-2': {'f': 0.35406512941748186, 'p': 0.5956987919724204, 'r': 0.2585699686276442}, 'rouge-l': {'f': 0.39629701595657746, 'p': 0.7077406122600427, 'r': 0.3595953228819612}} <br>\n",
    "------ VALIDATION SCORES ------ <br>\n",
    "means ==>  {'rouge-1': {'f': 0.45293486430462093, 'p': 0.7337109698177251, 'r': 0.33966474903256505}, 'rouge-2': {'f': 0.36218016025752886, 'p': 0.7158007184645797, 'r': 0.33119691474537416}, 'rouge-l': {'f': 0.32979074765465355, 'p': 0.6057084485814125, 'r': 0.2369253106378605}} <br>\n",
    "variations ==>  {'rouge-1': {'f': 0.0007295177929322034, 'p': 0.00033214824205945727, 'r': 0.00085176714178182}, 'rouge-2': {'f': 0.008965934099607477, 'p': 0.0006529253455923054, 'r': 0.000923471359297147}, 'rouge-l': {'f': 0.01589399125843298, 'p': 0.01671679368491213, 'r': 0.01140715934344113}} <br>\n",
    "\n",
    "Model 2: <br>\n",
    "------ TEST SCORES ------ <br>\n",
    "{'rouge-1': {'f': 0.419511775717035, 'p': 0.6278524033801669, 'r': 0.3230158028078079}, 'rouge-2': {'f': 0.2818223155601035, 'p': 0.4720717750614849, 'r': 0.2073344441552195}, 'rouge-l': {'f': 0.3444399179703348, 'p': 0.6080876388502864, 'r': 0.3130674264485994}} <br>\n",
    "------ VALIDATION SCORES ------ <br>\n",
    "means ==>  {'rouge-1': {'f': 0.38534570272519736, 'p': 0.6268919714401204, 'r': 0.28980828246072077}, 'rouge-2': {'f': 0.30576449426104646, 'p': 0.6053994427906307, 'r': 0.28027556399447767}, 'rouge-l': {'f': 0.2531938092218845, 'p': 0.46190996654299915, 'r': 0.18329398628165158}} <br>\n",
    "variations ==>  {'rouge-1': {'f': 0.0006976009215804269, 'p': 0.000165205406525276, 'r': 0.0007658420611953346}, 'rouge-2': {'f': 0.007030769662195063, 'p': 0.0006271341942744075, 'r': 0.0008567147825519877}, 'rouge-l': {'f': 0.018161723878091358, 'p': 0.027384267346398997, 'r': 0.012111137351717819}} <br>\n",
    "\n",
    "I choose model 1 because scores are higher and yet it is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
